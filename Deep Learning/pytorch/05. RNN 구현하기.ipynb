{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17137b5e-7e29-4376-a77f-bd7801b0ef4f",
   "metadata": {},
   "source": [
    "# IMBD 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d4ca3-35be-4250-8274-0ce2a1447d1f",
   "metadata": {},
   "source": [
    "https://github.com/groovallstar/pytorch_rnn_tutorial/blob/main/8_2_torchtext_migration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424b4ecb-ff33-4903-b813-b554725c4599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T01:59:52.134455Z",
     "start_time": "2024-03-18T01:59:43.717862Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter, test_iter = IMDB(root='.data', split=('train', 'test'))\n",
    "\n",
    "def train_valid_split(train_iterator, split_ratio=0.8, seed=42):\n",
    "    train_count = int(split_ratio * len(train_iterator))\n",
    "    valid_count = len(train_iterator) - train_count\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_set, valid_set = random_split(\n",
    "        train_iterator, lengths=[train_count, valid_count],\n",
    "        generator=generator)\n",
    "    return train_set, valid_set\n",
    "\n",
    "train_iter = to_map_style_dataset(train_iter)\n",
    "test_iter = to_map_style_dataset(test_iter)\n",
    "\n",
    "train_set, val_set = train_valid_split(train_iter)\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterator=yield_tokens(train_iter),\n",
    "    min_freq=5,\n",
    "    specials=['<unk>'],)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "    label_pipeline = lambda x: int(x)\n",
    "\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_tensor = pad_sequence(text_list, padding_value=1, batch_first=True)\n",
    "    return text_tensor, label_list\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(\n",
    "    val_set, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(\n",
    "    test_iter, batch_size=64, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7f97dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T01:59:52.149389Z",
     "start_time": "2024-03-18T01:59:52.136440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " 'I saW this film while at Birmingham Southern College in 1975, when it was shown in combination with the Red Balloon. Both films are similar in their dream-like quality. The bulk of the film entails a fish swimming happily in his bowl while his new owner, a little boy, is away at school. A cat enters the room where the fish and his bowl are, and begins to warily stalk his \"prey.\" The boy begins his walk home from school, and the viewer wonders whether he will arrive in time to save his fish friend. The fish becomes agitated by the cat\\'s presence, and finally jumps out of the bowl! The cat quickly walks over to the fish, gently picks him up with his paws, and returns him to his bowl. The boy returns happily to his fish, none the wiser.<br /><br />The ending is amazing in both its irony and its technical complexity. It is hard to imagine how the director could\\'ve pulled the technical feat back in 1959 -- it seems more a trick for 2003.<br /><br />If you can find it, watch it -- you won\\'t be disappointed! And if you *do* find it, let me know so I can get a copy, too!')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a418877b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T01:59:52.164377Z",
     "start_time": "2024-03-18T01:59:52.151378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966c3d0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T01:59:52.180036Z",
     "start_time": "2024-03-18T01:59:52.166377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 79, 391)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(val_dataloader), len(test_dataloader) # 배치 사이즈 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4248f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T01:59:52.195040Z",
     "start_time": "2024-03-18T01:59:52.184035Z"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2115e9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T01:59:52.210113Z",
     "start_time": "2024-03-18T01:59:52.198132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5565c41",
   "metadata": {},
   "source": [
    "# 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a569950e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T01:59:52.225042Z",
     "start_time": "2024-03-18T01:59:52.212038Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Gru(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        \"\"\"\n",
    "        n_layers: 히든 레이어 개수\n",
    "        hidden_dim: 히든 레이어 차원\n",
    "        n_vocab: 사전 사이즈\n",
    "        embed_dim: 임베딩된 데이터의 차원\n",
    "        n_classes: 레이블 수\n",
    "        dropout_p: 드랍아웃 비율\n",
    "        \"\"\"\n",
    "        super(Gru, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # 텍스트를 단어 단위인 토큰으로 벡터 변환\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # 아래 함수 참고\n",
    "        x, _ = self.gru(x, h_0) # GRU의 리턴 값(배치 사이즈, 입력값 길이, 히든 스테이트의 길이) -> 텐서 형태\n",
    "        h_t = x[:, -1, :] # 텐서로 크기가 변경, 마지막 히든 스테이트만 가져옴\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t) # 배치 사이즈와 히든 스테이트의 크기 -> 배치 사이즈의 출력층의 크기로 변환\n",
    "        return logit\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        \"첫 번째 히든 스테이트를 0 벡터로 초기화\"\n",
    "        weight = next(self.parameters()).data # 첫 가중치 데이터 추출(텐서)\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() # 현재 모델의 가중치와 같은 모양의 텐서의 값을 0으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc792114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T01:59:56.075210Z",
     "start_time": "2024-03-18T01:59:52.228036Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "EPOCHS = 10\n",
    "\n",
    "model = Gru(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff442bf",
   "metadata": {},
   "source": [
    "## 모델 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69490676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T02:02:57.541156Z",
     "start_time": "2024-03-18T01:59:56.078202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1] train loss: 0.7029 | val loss: 0.6939 val accuracy: 50.28%\n",
      "[Epoch:  2] train loss: 0.6938 | val loss: 0.6930 val accuracy: 50.36%\n",
      "[Epoch:  3] train loss: 0.6937 | val loss: 0.6947 val accuracy: 49.82%\n",
      "[Epoch:  4] train loss: 0.6919 | val loss: 0.6934 val accuracy: 49.60%\n",
      "[Epoch:  5] train loss: 0.7015 | val loss: 0.6991 val accuracy: 50.42%\n",
      "[Epoch:  6] train loss: 0.7097 | val loss: 0.6969 val accuracy: 49.72%\n",
      "[Epoch:  7] train loss: 0.6917 | val loss: 0.6931 val accuracy: 50.34%\n",
      "[Epoch:  8] train loss: 0.6899 | val loss: 0.6934 val accuracy: 49.78%\n",
      "[Epoch:  9] train loss: 0.6947 | val loss: 0.6927 val accuracy: 50.38%\n",
      "[Epoch: 10] train loss: 0.6861 | val loss: 0.6934 val accuracy: 49.86%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "best_val_loss = None\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for i, (text, label) in enumerate(train_dataloader):\n",
    "        text, label = text.to(DEVICE), label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        \n",
    "        label.data.sub_(1) # <unk>:0 인 token 값 제거\n",
    "        loss = f.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i == 100: break\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    val_loss_sum = 0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label in val_dataloader:\n",
    "            text, label = text.to(DEVICE), label.to(DEVICE)\n",
    "            label.data.sub_(1) # <unk>:0 인 token 값 제거\n",
    "            output = model(text)\n",
    "            val_loss_sum += f.cross_entropy(output, label, reduction='sum').item()\n",
    "            pred = output.max(1)[1].view(label.size()).data\n",
    "            val_correct += (pred == label.data).sum()\n",
    "\n",
    "    val_loss = val_loss_sum / len(val_set)\n",
    "    val_acc = val_correct / len(val_set) * 100\n",
    "    \n",
    "    print(f'[Epoch: {epoch:2d}] train loss: {loss.item():.4f} | val loss: {val_loss:.4f} val accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_model = model\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b23d5",
   "metadata": {},
   "source": [
    "## 최적 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea98ed26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T02:02:57.556149Z",
     "start_time": "2024-03-18T02:02:57.544080Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for i, (text, label) in enumerate(train_iter):\n",
    "        text, label = text.to(DEVICE), label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        \n",
    "        label.data.sub_(1) # <unk>:0 인 token 값 제거\n",
    "        loss = f.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    return loss\n",
    "\n",
    "def evaluate(model, val_iter, len_val_data):\n",
    "    model.eval()\n",
    "\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label in val_iter:\n",
    "            text, label = text.to(DEVICE), label.to(DEVICE)\n",
    "            label.data.sub_(1) # <unk>:0 인 token 값 제거\n",
    "            output = model(text)\n",
    "            loss_sum += f.cross_entropy(output, label, reduction='sum').item()\n",
    "            pred = output.max(1)[1].view(label.size()).data\n",
    "            correct += (pred == label.data).sum()\n",
    "\n",
    "    loss = loss_sum / len_val_data\n",
    "    acc = correct / len_val_data * 100\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7059db7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T02:02:57.571422Z",
     "start_time": "2024-03-18T02:02:57.558659Z"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.loss = float('inf')\n",
    "        self.patience = 0\n",
    "        self.patience_limit = patience\n",
    "        \n",
    "    def step(self, loss):\n",
    "        if self.loss > loss:\n",
    "            self.loss = loss\n",
    "            self.patience = 0\n",
    "        else:\n",
    "            self.patience += 1\n",
    "    \n",
    "    def is_stop(self):\n",
    "        return self.patience >= self.patience_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4e0941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T02:14:45.383410Z",
     "start_time": "2024-03-18T02:02:57.575426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1] train loss: 0.6921 | val loss: 0.6930 val accuracy: 50.46%\n",
      "[Epoch:  2] train loss: 0.6975 | val loss: 0.6929 val accuracy: 50.56%\n",
      "[Epoch:  3] train loss: 0.6968 | val loss: 0.6986 val accuracy: 50.50%\n",
      "[Epoch:  4] train loss: 0.3523 | val loss: 0.4016 val accuracy: 81.60%\n",
      "[Epoch:  5] train loss: 0.2616 | val loss: 0.2689 val accuracy: 89.28%\n",
      "[Epoch:  6] train loss: 0.0240 | val loss: 0.3060 val accuracy: 89.04%\n",
      "[Epoch:  7] train loss: 0.0034 | val loss: 0.3735 val accuracy: 89.14%\n",
      "[Epoch:  8] train loss: 0.0235 | val loss: 0.4406 val accuracy: 88.62%\n",
      "[Epoch:  9] train loss: 0.0025 | val loss: 0.4832 val accuracy: 88.92%\n",
      "[Epoch: 10] train loss: 0.1246 | val loss: 0.5812 val accuracy: 87.74%\n",
      "[Epoch: 11] train loss: 0.0154 | val loss: 0.5610 val accuracy: 88.80%\n",
      "[Epoch: 12] train loss: 0.0006 | val loss: 0.5824 val accuracy: 88.54%\n",
      "[Epoch: 13] train loss: 0.0009 | val loss: 0.5890 val accuracy: 89.04%\n",
      "[Epoch: 14] train loss: 0.0016 | val loss: 0.6209 val accuracy: 88.84%\n",
      "[Epoch: 15] train loss: 0.0006 | val loss: 0.6567 val accuracy: 89.10%\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "EPOCHS = 50\n",
    "\n",
    "model = Gru(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_model = None\n",
    "best_val_loss = None\n",
    "\n",
    "es = EarlyStopping(patience=10)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    loss = train(model, optimizer, train_dataloader)\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, val_dataloader, len(val_set))\n",
    "    \n",
    "    print(f'[Epoch: {epoch:2d}] train loss: {loss.item():.4f} | val loss: {val_loss:.4f} val accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    es.step(val_loss)\n",
    "    if es.is_stop():\n",
    "        break\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_model = model\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'bestmodel.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73520153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T07:07:50.019677Z",
     "start_time": "2024-03-15T07:07:49.986637Z"
    }
   },
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d89f799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T02:15:07.511176Z",
     "start_time": "2024-03-18T02:14:45.385433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.67 | test accuracy: 88.02\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(best_model, test_dataloader, len(test_iter))\n",
    "print(f'test loss: {test_loss:.2f} | test accuracy: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64103d0",
   "metadata": {},
   "source": [
    "# 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e57f25b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T02:15:30.361373Z",
     "start_time": "2024-03-18T02:15:07.513181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.27 | test accuracy: 88.51\n"
     ]
    }
   ],
   "source": [
    "best_model_state = torch.load('bestmodel.pt')\n",
    "best_model_loaded = Gru(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "best_model_loaded.load_state_dict(best_model_state)\n",
    "\n",
    "test_loss, test_acc = evaluate(best_model_loaded, test_dataloader, len(test_iter))\n",
    "print(f'test loss: {test_loss:.2f} | test accuracy: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77214c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
