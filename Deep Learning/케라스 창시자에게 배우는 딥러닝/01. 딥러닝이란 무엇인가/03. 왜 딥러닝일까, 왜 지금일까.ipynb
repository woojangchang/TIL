{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e87ca56",
   "metadata": {},
   "source": [
    "# 1.3 왜 딥러닝일까? 왜 지금일까?\n",
    "- 합성곱 신경망과 역전파는 1989년 소개, LSTM(Long Short-Term Memory) 알고리즘은 1997년 개발된 이후 변화가 거의 없음\n",
    "- 2012년 이후 딥러닝이 부상하게 된 변화 이유\n",
    "    - 하드웨어\n",
    "    - 데이터셋과 벤치마크(benchmark)\n",
    "    - 알고리즘 향상\n",
    "- 인터넷이 시작되며 데이터가 쌓이고, 게임 시장이 커지면서 고성능 그래픽 칩이 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31cd416",
   "metadata": {},
   "source": [
    "## 1.3.1 하드웨어\n",
    "- 1990년에 비하여 2010년 컴퓨터는 5천배 빨라짐\n",
    "- 2000년대 NVIDIA와 AMD 같은 회사에서 게임 그래픽 성능을 높이기 위해 대용량 고속 병렬 칩(그래픽 처리 장치(GPU))을 개발\n",
    "    - 2007년 NVIDIA에서 자사 GPU 제품을 위한 프로그래밍 인터페이스인 CUDA를 출시하면서 과학 커뮤니티가 혜택을 보게 됨\n",
    "    - 다양한 병렬 애플리케이션의 대형 CPU 클러스터가 소량의 GPU로 대체되기 시작\n",
    "    - 2011년 댄 크리슨과 알렉스 크리체브스키 등 일부 연구자들이 CUDA를 사용한 신경망 구현을 만들기 시작\n",
    "- 2016년 구글에서 텐서 처리 장치(Tensor Processing Unit; TPU)를 공개하며 심층 신경망을 실행하기 위해 새롭게 설계, GPU보다 10배 이상 빠르며 에너지 소비도 더 효율적임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ffb092",
   "metadata": {},
   "source": [
    "## 1.3.2 데이터\n",
    "- 저장 장치와 인터넷이 성장을 하며 딥러닝에 필요한 연료인 데이터셋을 수집, 배포할 수 있게 됨\n",
    "    - 플리커(Flickr)에서 사용자가 붙인 태그나 유튜브 영상 : 컴퓨터 비전의 데이터\n",
    "    - 위키피디아(Wikipedia) : 자연어 처리 분야의 데이터\n",
    "- ImageNet 데이터셋이 딥러닝의 성장을 이끈 촉매제 역할을 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e3df2",
   "metadata": {},
   "source": [
    "## 1.3.3 알고리즘\n",
    "- 깊게 쌓은 층을 통과해서 그래디언트(gradient)를 전파하는 것이 가장 큰 문제\n",
    "- 2009~2010년 알고리즘이 개선되면서 그래디언트가 더 잘 전파되게 됨\n",
    "    - 신경망의 층에 더 잘 맞는 활성화 함수(activation function)\n",
    "    - 층별 사전 훈련(pretraining)을 불필요하게 만든 가중치 초기화(weight initialization) 방법, Xavier 초기화 또는 Glorot 초기화라고 부름\n",
    "    - RMSProp과 Adam 같은 더 좋은 최적화 방법\n",
    "- 2014~2016년 배치 정규화(Batch Normalization), 잔차 연결(residual connection), 깊이별 분리 합성곱(depthwise separable convolution) 같은 고급 기술 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f97b1",
   "metadata": {},
   "source": [
    "## 1.3.4 새로운 투자의 바람\n",
    "- 벤처 캐피탈이 2011년 AI에 투자한 금액은 1,900만 달러에서 2014년 3억 9,400만 달러로 늘어남\n",
    "- 구글, 페이스북, 바이두, 마이크로소프트 등의 회사에서도 엄청난 투자를 하고 있으며 일부만 공개됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef68002",
   "metadata": {},
   "source": [
    "## 1.3.5 딥러닝의 대중화\n",
    "- 초창기 딥러닝을 하려면 C++과 CUDA의 전문가가 되어야 했음\n",
    "- 지금은 씨아노(Theano)와 텐서플로(TensorFlow)가 개발되며 기본 파이썬 스크립트 기술만 있으면 고수준의 딥러닝을 연구하는 데 충분함\n",
    "- 2015년 케라스 같은 사용자 편의 도구들도 등장하며 새로운 딥러닝의 주력 솔루션이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad189b9",
   "metadata": {},
   "source": [
    "## 1.3.6 지속될까?\n",
    "- 미래에 딥러닝을 사용하지 않을 수 있으나 파생된 것들은 사용할 것\n",
    "    - **단순함** : 특성 공학이 필요하지 않아 복잡하고 불안정한 과정을 거치지 않고 모델로 바꾸어 줌\n",
    "    - **확장성** : GPU 또는 TPU에서 쉽게 병렬화할 수 있기 때문에 무어의 법칙 혜택을 크게 볼 수 있음\n",
    "        - 작은 배치(batch) 데이터에서 반복적으로 훈련되기 때문에 어떤 크기의 데이터셋에서도 훈련될 수 있음\n",
    "    - **다용도와 재사용성** : 기존 머신 러닝과 달리 처음부터 다시 학습하지 않고 추가되는 데이터로도 훈련할 수 있음\n",
    "        - 온라인 학습(online learning)이 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc29f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
