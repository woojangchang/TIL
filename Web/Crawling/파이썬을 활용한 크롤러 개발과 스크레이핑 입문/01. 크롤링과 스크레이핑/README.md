# 01. 크롤링과 스크레이핑

## 크롤러와 크롤링

- **크롤러** : 자동으로 웹 페이지에 있는 정보를 수집하는 프로그램으로 봇, 로봇, 스파이더 등으로도 불림
  - 대규모의 정보를 단시간에 수집

## 스크레이핑이란?

- 크롤러가 정보를 수집한다면 스크레이핑은 수집한 정보를 분석해서 필요한 정보를 추출하는 것
- 크롤링과 스크레이핑의 흐름
  - 수집 > 분석 > 추출 > 가공 > 저장 > 출력

# 02. Wget으로 시작하는 크롤러 개발

기본적인 명령어를 사용해서 크롤러 만들기

## Wget

- Wget : HTTP/FTP를 사용해서 서버에서 파일을 내려받기 위한 오픈소스 소프트웨어
  - 웹 페이지를 재귀적으로 순회하며 내려받기
  - HTML 링크를 상대 경로에서 절대 경로로 변환
  - 특정 확장자만 내려받기
  - 내려 받을 간격을 설정 등의 여러 기능을 지니고 있음
- 설치
  - `which wget` 명령어를 입력했을 때 경로가 출력되면 설치된 것
  - `yum install wget` 또는 `apt-get install wget`으로 리눅스에 설치
- 사용 방법
  - `wget <내려 받고 싶은 파일의 URL>` : 기존 파일명 그대로 다운로드
  - `wget -O <저장하고 싶은 파일 경로와 이름> <내려 받고 싶은 파일의 URL>` : 위치와 파일 이름을 지정
- 파일을 저장하지 않고 내려받기
  - `wget -O - <page url>` : 표준 출력으로 출력
    - 표준 출력 장점 : 파이프(명령어의 표준 출력을 다음 명령어로 전달하는 처리)를 사용해 다른 명령어로 내려받은 내용을 넘겨서 처리할 수 있다는 것
- 그 외 명령어 옵션
  - -h : 도움말 출력
  - -q : 진행 정보 메시지 출력하지 않음
  - -o : 진행 정보 메시지를 저장
  - -O : 파일에 저장
  - -c : 이전에 내려받던 것을 재개
  - -t : 재실행 횟수를 지정
  - -r : 재귀적으로 링크를 타고 돌며 내려받음
  - -l : 재귀적으로 내려받을 때 링크를 순회하는 깊이 지정
  - -w : 재귀적으로 내려받을 때 내려받을 간격 지정
  - -np : 재귀적으로 내려받을 때 지정한 디렉터리만 순회
  - -N : 파일이 변경됐을 대만 내려받음
  - --user=<사용자 이름> : 사용자 이름 지정
  - --password=<비밀번호> : 비밀번호 지정
  - --referer=\<URL> : 레퍼러 지정
  - --spider : 파일을 내려받지는 않고 존재하는지 확인만 함
  - -A : 지정한 확장자의 파일만 내려받음
  - -k : 링크와 이미지 등의 참조를 절대 경로에서 상대 경로로 변환

- 재귀적으로 내려받기
  - 내려받은 파일에 링크가 표함돼 있을 때, 해당 링크 대상을 순회하며 내려받는 것을 의미
  - 링크 대상에 추가로 링크가 또 있다면 그러한 링크 대상도 내려받음
  - `wget -r -l1 http://example.com/` : 1단계만 순회
    - `-l` 옵션을 주지 않으면 5단계까지 순회
  - `wget -r -l1 -w3 http://example.com/` : 3초 간격으로 다운 받음
  - `wget -r -l1 -A jpg,png,gif http://example.com/` : jpg, png, gif 확장자를 가진 파일만 다운 받음
  - `wget -r -l1 -R jpg,png,gif http://example.com/` : jpg, png, gif 확장자를 가진 파일을 제외하고 다운 받음
  - `wget -r -np http://example.com/camera` : 부모 디렉터리를 내려받기 대상에서 제외
- `sudo apt-get install tree`로 tree를 설치한 후 `tree example.com/`으로 디렉터리 구성 확인

# 03. 유닉스 명령어

- `|`를 사용하면 어떤 명령어의 처리 결과를 다른 명령어로 넘길 수 있음
- `grep <패턴> <파일 이름(여러 개 지정 가능)>` : 패턴에 맞는 줄을 찾아 출력



