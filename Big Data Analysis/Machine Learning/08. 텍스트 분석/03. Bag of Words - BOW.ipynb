{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12d1728",
   "metadata": {},
   "source": [
    "# BOW\n",
    "- 문맥, 순서를 무시하고 단어의 빈도 값으로 피처 값을 추출하는 모델\n",
    "\n",
    "## 알고리즘\n",
    "1. 각 문장에 있는 모든 단어에서 중복을 제거하고 각 단어를 칼럼 형태로 나열한 후 각 단어에 고유 인덱스 부여\n",
    "2. 개별 문장에서 해당 단어가 나타나는 횟수(Occurrence)를 각 단어 인덱스에 기재\n",
    "\n",
    "## 장점\n",
    "- 쉽고 빠른 구축\n",
    "- 문서의 특성을 잘 나타낸다는 경험으로 활용도가 높음\n",
    "\n",
    "## 단점\n",
    "- 문맥 의미(Semantic Context) 반영 부족\n",
    "    - 단어의 순서를 고려하지 않음\n",
    "    - n_gram 기법을 활용할 수 있으나 제한적임\n",
    "- 희소 행렬 문제(희소성, 희소 행렬)\n",
    "    - 단어의 개수는 수만~수십만 인데 비하여(column 수) 각 문장에 나타나는 단어의 수는 적기 때문에 데이터에 0이 많음\n",
    "    - 희소 행렬은 ML의 성능을 떨어뜨림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37e9b4",
   "metadata": {},
   "source": [
    "## BOW 피처 벡터화\n",
    "- M개의 텍스트 문서, 총 N개의 단어가 있을 경우 M x N개의 단어 피처로 이뤄진 행렬을 구성\n",
    "    - column : 단어 feature들\n",
    "    - row : 각 문서의 단어 빈도/정규화 변환된 횟수\n",
    "- 카운트 기반의 벡터화\n",
    "    - 해당 단어가 나타나는 횟수\n",
    "    - 언어의 특성상 문장에서 자주 사용하는 단어에 높은 값을 부여한다는 단점이 있음\n",
    "- TF-IDF(Term Frequency Inverse Document Frequency) 벡터화\n",
    "    - 모든 문서에서 전반적으로 자주 나타나는 단어에 페널티를 주는 방식\n",
    "    - $\\text{TFIDF}_i = \\text{TF}_i \\times \\log{\\dfrac{N}{\\text{DF}_i}}$\n",
    "    - $\\text{TF}_i$ : 개별 문서에서의 단어 $i$ 빈도\n",
    "    - $\\text{DF}_i$ : 단어 $i$를 가지고 있는 문서 개수\n",
    "    - $N$ : 전체 문서 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc50423",
   "metadata": {},
   "source": [
    "## 사이킷런 `CountVectorizer`, `TfidfVectorizer`\n",
    "\n",
    "- 소문자 일괄 변환, 토큰화, 스톱 워드 필터링 등의 데이터 전처리도 함께 수행\n",
    "- fit, transform을 통해 객체 반환\n",
    "- 둘 모두의 파라미터는 동일함\n",
    "\n",
    "### 파라미터\n",
    "- max_df : 자주 나타나는 단어를 제외하기 위한 파라미터\n",
    "    - 정수 값을 주면 그 단어 이하의 단어만 피처로 추출\n",
    "    - 부동소수 값을 주면 해당 빈도수 이하의 단어만 피처로 추출\n",
    "- min_df : 너무 적게 나타나는 단어를 제외하기 위한 파라미터 - 가비지성 단어일 확률이 높음\n",
    "    - 정수 값을 주면 그 단어 이상의 단어만 피처로 추출\n",
    "    - 부동소수 값을 주면 해당 빈도수 이상의 단어만 피처로 추출\n",
    "- max_features : 추출하는 피처 개수 제한\n",
    "    - 가장 높은 빈도를 가지는 단어 순으로 자름\n",
    "- stop_words : 'english'로 지정하면 영어의 스톱 워드를 추출하지 않음\n",
    "- n_gram_range : n_gram 범위 설정. 튜플 단위 (범위 최솟값, 범위 최댓값)을 지정\n",
    "    - (1, 1)로 지정하면 토큰화된 단어를 1개씩 피처로 추출\n",
    "    - (1, 2)로 지정하면 토큰화된 단어를 1개씩, 순서대로 2개씩 묶어서 피처로 추출\n",
    "- analyzer : 피처 추출을 수행할 단위. 기본값은 word이며 character의 특정 범위를 피처로 만드는 특정한 경우에 사용\n",
    "- token_pattern : 토큰화 수행하는 정규 표현식 패턴.\n",
    "    - 기본값 : '\\b\\w\\w+\\b'로 공백 또는 개행 문자 등으로 구분된 단어 분리자(\\b) 사이의 두 문자 이상(\\w\\w+)의 단어를 토큰으로 분리\n",
    "- tokenizer : 별도의 토큰화 함수를 이용시 적용\n",
    "\n",
    "### 알고리즘\n",
    "1. 사전 데이터 가공 : 모든 문자를 소문자로 변경 (lowercase=True)\n",
    "2. 토큰화 : n_gram_range를 반영하여 각 단어를 토큰화, 단어 기준 (analyzer=True)\n",
    "3. 텍스트 정규화 : Stop Words 필터링만 수행하며 Stemmer, Lemmatize는 지원되지 않으므로 외부 패키지로 미리 Text Normalization 수행 필요\n",
    "4. 피처 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53922361",
   "metadata": {},
   "source": [
    "## BOW 벡터화를 위한 희소 행렬\n",
    "- BOW 형태를 가진 언어 모델의 피처 벡터화의 대부분이 희소 행렬\n",
    "- 불필요한 0 값이 메모리 공간에 할당되어 메모리 공간이 많이 필요\n",
    "- 행렬의 크기가 커서 연산 시 시간이 많이 소모됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e72fe",
   "metadata": {},
   "source": [
    "## 희소 행렬 - COO 형식\n",
    "- COO(Coordinate; 좌표) 형식 : 0이 아닌 데이터만 별도의 데이터 배열에 저장하고 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식\n",
    "- [[3, 0, 1], [0, 2, 0]] → (0, 0), (0, 2), (1, 1) → row : [0, 0, 1], column : [0, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0922d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3, 0, 1], [0, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c96e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "# 행, 열 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용해 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79cf6b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원래 형태의 행렬로 전환\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7978a8",
   "metadata": {},
   "source": [
    "## 희소 행렬 - CSR 형식\n",
    "- CSR(Compressed Sparse Row)\n",
    "- 행 위치 배열 [0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5]를 CSR로 변환하면 [0, 2, 7, 9, 10, 12]로 연속해서 나오는 숫자의 첫번째 위치값만 뽑아낸 뒤 마지막에 데이터의 총 항목 개수를 배열에 추가(이 경우 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe3e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                  [1, 4, 0, 3, 2, 5],\n",
    "                  [0, 6, 0, 3, 0, 0],\n",
    "                  [2, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 7, 0, 8],\n",
    "                  [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행, 열 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CRS 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabedc82",
   "metadata": {},
   "source": [
    "### 실제로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf3b4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO로 변환된 데이터\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "CSR로 변환된 데이터\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "dense3 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                  [1, 4, 0, 3, 2, 5],\n",
    "                  [0, 6, 0, 3, 0, 0],\n",
    "                  [2, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 7, 0, 8],\n",
    "                  [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)\n",
    "\n",
    "print('COO로 변환된 데이터')\n",
    "print(coo)\n",
    "print('CSR로 변환된 데이터')\n",
    "print(csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddeb262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
