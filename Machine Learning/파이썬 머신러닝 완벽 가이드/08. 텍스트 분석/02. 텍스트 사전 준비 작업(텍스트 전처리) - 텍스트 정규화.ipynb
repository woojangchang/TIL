{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b1c4ec",
   "metadata": {},
   "source": [
    "# 텍스트 정규화\n",
    "- 클렌징\n",
    "- 토큰화\n",
    "- 필터링/스톱 워드 제거/철자 수정\n",
    "- Stemming\n",
    "- Lematization\n",
    "\n",
    "## 클렌징\n",
    "- HTML, XML 태그나 특정 기호 등을 제거하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e64d9",
   "metadata": {},
   "source": [
    "## 텍스트 토큰화\n",
    "- 문장 토큰화 : 문서에서 문장을 분리\n",
    "- 단어 토큰화 : 문장에서 단어를 분리\n",
    "\n",
    "### 문장 토큰화\n",
    "- 마침표(.), 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적\n",
    "- 정규표현식으로도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee2e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "['Python is a programming language that lets you work more quickly and integrate your systems more effectively.', 'You can learn to use Python and see almost immediate gains in productivity and lower maintenance costs.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "# 최초 1회만 실행하여 다운받으면 이후에는 실행할 필요 없음\n",
    "nltk.download('punkt') # 단어 사전\n",
    "\n",
    "text_sample = 'Python is a programming language that lets you work more quickly and integrate your systems more effectively. \\\n",
    "                You can learn to use Python and see almost immediate gains in productivity and lower maintenance costs.'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d89d3d",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "- Word Tokenization : 공백, 콤마, 마침표, 개행문자 등으로 분리하지만 정규표현식으로 다양한 방식으로 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05eff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 36\n",
      "['Python', 'is', 'a', 'programming', 'language', 'that', 'lets', 'you', 'work', 'more', 'quickly', 'and', 'integrate', 'your', 'systems', 'more', 'effectively', '.', 'You', 'can', 'learn', 'to', 'use', 'Python', 'and', 'see', 'almost', 'immediate', 'gains', 'in', 'productivity', 'and', 'lower', 'maintenance', 'costs', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'Python is a programming language that lets you work more quickly and integrate your systems more effectively. \\\n",
    "                You can learn to use Python and see almost immediate gains in productivity and lower maintenance costs.'\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d32ce0",
   "metadata": {},
   "source": [
    "### 문장 별 단어 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f04f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "[['Python', 'is', 'a', 'programming', 'language', 'that', 'lets', 'you', 'work', 'more', 'quickly', 'and', 'integrate', 'your', 'systems', 'more', 'effectively', '.'], ['You', 'can', 'learn', 'to', 'use', 'Python', 'and', 'see', 'almost', 'immediate', 'gains', 'in', 'productivity', 'and', 'lower', 'maintenance', 'costs', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # 문장 분리\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 단어 분리\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b085177",
   "metadata": {},
   "source": [
    "### n-gram\n",
    "- \"Python is a programming language\"를 2-gram(bigram)으로 만들면 (Python, is), (is, a), (a, programming), (programming, language)와 같이 연속적으로 2개의 단어들을 순차적으로 이동하면서 토큰화\n",
    "- 문맥적인 의미가 무시되는 것을 해결하고자 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac3a79",
   "metadata": {},
   "source": [
    "## 스톱 워드 제거\n",
    "- 분석에 큰 의미가 없는 단어\n",
    "- 영어의 is, the, a, will 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c346349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # 스톱 워드\n",
    "\n",
    "print('영어 stop words 개수:', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd3d5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'programming', 'language', 'lets', 'work', 'quickly', 'integrate', 'systems', 'effectively', '.'], ['learn', 'use', 'python', 'see', 'almost', 'immediate', 'gains', 'productivity', 'lower', 'maintenance', 'costs', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    \n",
    "    # 각 문장별 단어에 대해 스톱 워드 제거\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ade783",
   "metadata": {},
   "source": [
    "## Stemming과 Lemmatization\n",
    "- Stemming과 Lemmatization 모두 변형된 단어(영어의 경우 과거/현재형 등)의 원형을 찾는 것\n",
    "    - Lemmatization이 문법적인 요소와 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아줌\n",
    "    - 대신 시간이 더 오래 걸림\n",
    "- NLTK의 Stemmer와 Lemmatizer\n",
    "    - Stemmer : Porter, Lancaster, Snowball Stemmer\n",
    "    - Lemmatizer : WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f27d9",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer\n",
    "- es(3인칭 단수), ed(과거형) 등을 제대로 인식하지 못함 (amuses > amus, amused > amus)\n",
    "- 비교급 또한 제대로 찾지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62895afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3f925",
   "metadata": {},
   "source": [
    "### WordNetLemmatizer\n",
    "- 정확한 원형 단어 추출을 위해 단어의 품사를 입력해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9325712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a1ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
